{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brittany-city/brittany_bot/blob/main/gpt2_brittanybot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup"
      ],
      "metadata": {
        "id": "j99bHYOaZ_Gk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "outputId": "3be42d95-4b9f-4167-b97c-5529996b91d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XBAYvA0Dw4K",
        "outputId": "376a808d-c6f0-4200-a9b4-cbcf53adba8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD4rHWf6b_kZ",
        "outputId": "05522fe0-6fe9-4c94-add3-e40657c6c92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "outputId": "016e24f5-c042-444c-9b3d-e1f80208b0b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 23 18:40:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    29W /  70W |    326MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1941      C                                     323MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 Model"
      ],
      "metadata": {
        "id": "iBxfiot4aFjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT2 model has 117M, 124M, or 355M / 355M took a while\n",
        "model_name = \"124M\""
      ],
      "metadata": {
        "id": "l1-vmpgXKDE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model if it is not present\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "    print(f\"Downloading {model_name} model...\")\n",
        "    gpt2.download_gpt2(model_name=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1A7uTYxKJ5H",
        "outputId": "c02f382b-e186-4f84-8af8-4b391e2dbb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 124M model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 387Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 1.06Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 320Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:49, 10.0Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 351Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.45Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.42Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a Tensorflow session for GPT2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "rGEjC9QJKxHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of steps for model to take\n",
        "num_steps = 100"
      ],
      "metadata": {
        "id": "VWH0ruknKMbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no9cy0dCL0Hm",
        "outputId": "596e1a88-e4d0-4a8e-e4d7-f817e53d45e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file path of tweets\n",
        "file_name = \"clean_tweets.csv\""
      ],
      "metadata": {
        "id": "pqk_7yszKm_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "metadata": {
        "id": "sR5EfMlvLbhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass in the session\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              steps=num_steps,\n",
        "              #restore_from='fresh',\n",
        "              restore_from='latest',\n",
        "              overwrite = True,\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=500,\n",
        "              save_every=500\n",
        "              )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2LdbIz-Kqzv",
        "outputId": "5afdbd15-e7c6-4e74-d36a-d92fc856d4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 149.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 39399 tokens\n",
            "Training...\n",
            "Saving checkpoint/run1/model-0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 | 29.37] loss=0.07 avg=0.07\n",
            "[20 | 52.64] loss=0.07 avg=0.07\n",
            "[30 | 76.25] loss=0.05 avg=0.06\n",
            "[40 | 99.41] loss=0.05 avg=0.06\n",
            "[50 | 122.50] loss=0.04 avg=0.06\n",
            "[60 | 145.81] loss=0.04 avg=0.06\n",
            "[70 | 169.13] loss=0.04 avg=0.05\n",
            "[80 | 192.37] loss=0.04 avg=0.05\n",
            "[90 | 215.60] loss=0.04 avg=0.05\n",
            "[100 | 238.84] loss=0.02 avg=0.05\n",
            "Saving checkpoint/run1/model-100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/checkpoint/run1* /content/models/124M/"
      ],
      "metadata": {
        "id": "YrGbyjuAYcwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/models/124M/'"
      ],
      "metadata": {
        "id": "7-qjOTat9rBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_path = '/content/drive/MyDrive/generated_tweets'"
      ],
      "metadata": {
        "id": "ZZQEf5wq_oE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')\n",
        "print(time.ctime(time.time()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlj250HfjvuL",
        "outputId": "e3fd81e1-0c03-4e3d-d0bc-ec2bda733563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 23 18:57:07 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt2 generate new tweets -- save to specific file\n",
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=140,\n",
        "                      temperature=0.9,\n",
        "                      prefix=\"<|startoftext|\",\n",
        "                      top_k=0,\n",
        "                      top_p=0.9,\n",
        "                      truncate='<|endoftext|>',\n",
        "                      nsamples=5,\n",
        "                      batch_size=5\n",
        "                      )"
      ],
      "metadata": {
        "id": "X7f5r3DxmXHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "outputId": "89a1fb54-c67c-49b5-a614-83cd4724e869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|>959<|endoftext|>\n",
            "<|startoftext|>960<|endoftext|>\n",
            "<|startoftext|>961<|endoftext|>\n",
            "<|startoftext|>963<|endoftext|>\n",
            "<|startoftext|>964<|endoftext|>\n",
            "<|startoftext|>965<|endoftext|>\n",
            "<|startoftext|>966<|endoftext|>\n",
            "<|startoftext|>969<|endoftext|>\n",
            "<|startoftext|>970<|endoftext|>\n",
            "<|startoftext|>971<|endoftext|>\n",
            "<|startoftext|>972<|endoftext|>\n",
            "<|startoftext|>973<|endoftext|>\n",
            "<|startoftext|>974<|endoftext|>\n",
            "<|startoftext|>975<|endoftext|>\n",
            "<|startoftext|>978<|endoftext|>\n",
            "<|startoftext|>979<|endoftext|>\n",
            "<|startoftext|>980<|endoftext|>\n",
            "<|startoftext|>981<|endoftext|>\n",
            "<|startoftext|>982<|endoftext|>\n",
            "<|startoftext|>983<|endoftext|>\n",
            "<|startoftext|>984<|endoftext|>\n",
            "<|startoftext|>985<|endoftext|>\n",
            "<|startoftext|>986<|endoftext|>\n",
            "<|startoftext|>988<|endoftext|>\n",
            "<|startoftext|>989<|endoftext|>\n",
            "<|startoftext|>990<|endoftext|>\n",
            "<|startoftext|>991<|endoftext|>\n",
            "<|startoftext|>992<|endoftext|>\n",
            "<|startoftext|>993<|endoftext|>\n",
            "<|startoftext|>994<|endoftext|>\n",
            "<|startoftext|>995<|endoftext|>\n",
            "<|startoftext|>996<|endoftext|>\n",
            "<|startoftext|>997<|endoftext|>\n",
            "<|startoftext|>998<|endoftext|>\n",
            "<|startoftext|>999<|endoftext|>\n",
            "<|startoftext|>1000<|endoftext|>\n",
            "<|startoftext|>1002<|endoftext|>\n",
            "<|startoftext|>1004<|endoftext|>\n",
            "<|startoftext|>1005<|endoftext|>\n",
            "<|startoftext|>1006<|endoftext|>\n",
            "<|startoftext|>1008<|endoftext|>\n",
            "<|startoftext|>1009<|endoftext|>\n",
            "<|startoftext|>1010<|endoftext|>\n",
            "<|startoftext|>1011<|endoftext|>\n",
            "<|startoftext|>1016<|endoftext|>\n",
            "<|startoftext|>1017<|endoftext|>\n",
            "<|startoftext|>1018<|endoftext|>\n",
            "<|startoftext|>1016<|endoftext|>\n",
            "<|startoftext|>1019<|endoftext|>\n",
            "<|startoftext|>1020<|endoftext|>\n",
            "<|startoftext|>1021<|endoftext|>\n",
            "<|startoftext|>1022<|endoftext|>\n",
            "<|startoftext|>1023<|endoftext|>\n",
            "<|startoftext|>1024<|endoftext|>\n",
            "<|startoftext|>1025<|endoftext|>\n",
            "<|startoftext|>1029<|endoftext|>\n",
            "<|startoftext|>1030<|endoftext|>\n",
            "<|startoftext|>1032<|endoftext|>\n",
            "<|startoftext|>1033<|endoftext|>\n",
            "<|startoftext|>1034<|endoftext|>\n",
            "<|startoftext|>1035<|endoftext|>\n",
            "<|startoftext|>10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TihRskX9pqX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}